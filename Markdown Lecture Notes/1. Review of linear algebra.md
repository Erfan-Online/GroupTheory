# Review of Linear Algebra

**$n \times n$ Matrices**:
$$
\begin{pmatrix}
a_{11} & a_{12} & ...  & a_{1n} \\
... & ... & ... & ... \\
... & ... & a_{ij} & ...\\\ \\
a_{n1} & a_{n2} & ...  & a_{nn} \\
\end{pmatrix}
$$

For now, we only deal with Real matrices, i.e. $a_{ij} \in \mathbb{R}$

**Notation**: \
$M_n(\mathbb{R})$ is the set of all $n \times n$ matrices with entries in $\mathbb{R}$, which makes up the $\mathbb{R}$ vector space of dimension $n^2$.

Denoting the entries of $A$ with $a_{ij}$ and the entries of $B$ with $b_{ij}$,

We can **add** these matrices:
$$
(A + B)_{ij} = A_{ij} + B_{ij}
$$

We can do **scalar multiplication**:
$$
\alpha \in \mathbb{R}, \quad (\alpha A)_{ij} = \alpha \times A_{ij}
$$

We can **multiply matrices**:
$$
(A \times B)_{ij} = \sum_k a_{ik} b_{kj}
$$
In other words, $(A \times B)_{ij}$ is the dot product of the $i$-th row of $A$ with the $j$-th column of $B$.

**Note**: Matrices are linear operators.

If $A$ represents $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ and $B$ represents $S: \mathbb{R}^n \rightarrow \mathbb{R}^n$, then $AB$ represents the composition $T\circ S$

_Addition_ of matrices is **commutative**:
$$
A + B = B + A
$$

But _multiplication_ is **not commutative**: 
(might be true in some cases, but not in general)

For example,
$$
\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
\begin{pmatrix}
0 & 0 \\
0 & 1
\end{pmatrix} = 
\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
$$
but,
$$
\begin{pmatrix}
0 & 0 \\
0 & 1
\end{pmatrix} 
\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix} = 
\begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix} = \textbf{0}
$$

The $\begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$ matrix is the **zero element** of the $M_n(\mathbb{R})$ as a vector space, meaning that 
$$
\textbf{0} + A = A + \textbf{0} = A
$$

The **identity** matrix is defined as:
$$
I = \begin{pmatrix}
1 & 0 & ... & 0 \\
0 & 1 & ... & 0 \\
... & ... & ... & ... \\
0 & ... & ... & 1
\end{pmatrix}
$$
The identity matrix is the **unit element** of $M_n(\mathbb{R})$, meaning that 
$$
AI = IA = A
$$

**Distributive Law**:
$$
(A + B) C = AC + BC
$$

**Associativity**:
$$
(A  B) C = A (B  C)
$$


**Note**:
To prove associativity, reinterpret matrices as linear transformations. The result then follows from associativity of compositions.

## Invertible Matrices

We say $A$ is **invertible** $\leftrightarrow$ $\exists$ a matrix $B$ such that $AB = BA = I$.

**Notes**:
1. Not all matrices are invertible. 
For example, $0$ cannot be invertible, since $\textbf{0}A = \textbf{0} = A \textbf{0}$.

2. $I$ is invertible, since $II = II^{-1} = I$.

3. A $1\times1$ matrix is invertible, $\leftrightarrow$ $a \neq 0$. 
($A^{-1} = \frac{1}{A}$)

4. $2 \times 2$ matrices $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$ are invertible, $\leftrightarrow det(A) = ad - bc \neq 0$. \
Then, $A^{-1} = \frac{1}{det(A)} \begin{pmatrix} d & -c \\ -b & a \end{pmatrix}$.
$\begin{pmatrix} d & -c \\ -b & a \end{pmatrix}$ is called the matrix of _cofactors_.

5. If the inverse of a matrix exists, it is **unique**.
    _Proof_: 
    $$
    \begin{align*}
    \text{Suppose} &\quad AB = AC = I, \\
    \rightarrow &\quad 
    B(AB) = B(AC) \\
    \quad &=(BA)B = (BA)C  \\
    \quad &= IB = IC \\
    \quad &= B = C
    \end{align*}
    $$

<br/>